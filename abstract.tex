%!TEX root=thesis.tex

Semantic segmentation of satellite images is an important task as it has a wide range of practical applications, such as land cover mapping, poolution monitoring, deforestation detection and economic assessment. With the rise of deep learning, semantic segmentation models have been built using the convolutional neural network for many years. CNNs have proven to be really good at capturing local features but limited when capturing the global features. Recently, Transformers, a network that was originally designed for processing texts were adopted by the field of computer vision and has shown a lot of potential to computer vision task such as semantic segmentation. Unlike CNN, Transformer has the ability to capture both local and global features. For FYP 1, we utilized UNet-like Transformer (UNetFormer) that is made up of a CNN-based encoder and Transformer-based decoder for semantic segmentation of satellite images to get an initial result. We also trained a U-Net, a purely CNN-based encoder-decoder network using the same dataset for comparison purpose. The dataset chosen for this project is the LoveDA dataset which contains 5987 High Spatial Resolution (HSR )images with 166768 annotated pixels from three different cities in China.

The U-Net model gave an mIoU of 33.6\% and the UNetFormer gave an mIoU gave an mIoU of 72.5\%. U-Net has only 2.4 million parameters compared to 11.2 million on the UNetFormer.

