%!TEX ROOT = thesis.tex
\chapter{METHODOLOGY}

The implementation of this project is split into five different steps namely Data Collection, Data Preprocessing, Model Design and Implementation, Model Evaluation and Model Refinement. The general flow of the research methodology can be seen in Figure \ref{fig:flowchart1}.

\FloatBarrier
\begin{figure}[!h]
\includegraphics[width=6.5cm, height=8.5cm]{images/flowchart1.png}
\centering
\caption{Flowchart of Proposed Research Methodology}
\label{fig:flowchart1}
\end{figure}
\FloatBarrier

\section{Data Collection}

There were 12 different datasets considered at the beginning of this project and their details can be found in Chapter 2. In the end, we decided to choose LoveDA dataset \cite{loveda} because of several reasons. The first reason is it has one of the lowest spatial resolution with 0.3m. The second reason is it has a total of 5,987 samples which we considered as a good number of samples. Te third reason is the images in LoveDA dataset is actually collected using satellites unlike several other dataset that is a mixed of images collected through satellites and drones. The final reason is the images on LoveDA comes in PNG format which is much easier to work with compared to some other dataset that comes in GeoTIFF format. Although GeoTIFF carry more information, we concluded that the time and effort required to process and train models using GeoTIFF format is too much.

LoveDA dataset was introduced in Chapter 2 but here we are going to give an in-depth statistical analysis on the dataset. Table \ref{tab:class-loveda} gives the description of the classes in the dataset.

\begin{table}[!h]
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Classes}} & \multicolumn{1}{c|}{\textit{\textbf{Description}}}                                                                                \\ \hline
\textit{\textbf{Background}}           & Any objects that don't really belong in the rest of the class.                                                                    \\ \hline
\textit{\textbf{Building}}             & Include objects such as houses, schools, farmhouses                                                                               \\ \hline
\textit{\textbf{Road}}                 & Include tar roads and dirt roads.                                                                                                 \\ \hline
\textit{\textbf{Water}}                & Include rivers, lakes, sea, pools and drains.                                                                                     \\ \hline
\textit{\textbf{Barren}}               & \begin{tabular}[c]{@{}l@{}}Unused land that is not used for any residential,\\ commercial or agriculture activities.\end{tabular} \\ \hline
\textit{\textbf{Forest}}               & Forest.                                                                                                                           \\ \hline
\textit{\textbf{Agriculture}}          & Farms.                                                                                                                            \\ \hline
\end{tabular}
\caption{Classes Description for LoveDA Dataset.}
\label{tab:class-loveda}
\end{table}
\FloatBarrier

Each pixel in the dataset is labelled as it is made specifically for semantic segmentation task. The images in LoveDA dataset are separated into two groups; the rural images and the urban images. The sample images and maasks are shown in Figure \ref{fig:6} and \ref{fig:1564}. The corresponding colour to each class in the mask is shown in Table \ref{tab:mask-loveda}. 
\FloatBarrier
\begin{figure}[!htb]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth, height=0.35\textheight]{images/6.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth, height=0.35\textheight]{images/mask-rural-6.png}
    \end{minipage}
\caption{An Image and Mask From The Rural Set \protect\cite{loveda}}
\label{fig:6}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[!htb]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth, height=0.35\textheight]{images/1564.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth, height=0.35\textheight]{images/mask-urban-1564.png}
    \end{minipage}
\caption{An Image and Mask From The Urban Set \protect\cite{loveda}}
\label{fig:1564}
\end{figure}
\FloatBarrier

\begin{table}[]
\centering
\begin{tabular}{|l|l}
\hline
\multicolumn{1}{|l|}{\textbf{Class}} & \multicolumn{1}{l|}{\textbf{Colour}} \\ \hline
       Agriculture & \cellcolor[HTML]{ffff00}             \\\cline{1-1}
       Forest & \cellcolor[HTML]{76ee00}             \\\cline{1-1}
       Road & \cellcolor[HTML]{c1cdcd}             \\\cline{1-1}
        Water & \cellcolor[HTML]{0000ff}             \\\cline{1-1}
        Building & \cellcolor[HTML]{ff4040}             \\\cline{1-1}
       Background & \cellcolor[HTML]{000000}             \\\cline{1-1}
       Barren & \cellcolor[HTML]{8a360f}            \\ \cline{1-1}
\end{tabular}
\caption{Classes Colour for LoveDA Mask.}
\label{tab:mask-loveda}
\end{table}
\FloatBarrier

A training set, a validation set, and a testing set have previously been created for each group. To be more precise, the training set consists of 2522 photos, the validation set of 1669 images, and the testing set of 1796 images. The structure of the dataset is shown in figure \ref{fig:loveda-structure} .

\FloatBarrier
\begin{figure}[!h]
\includegraphics[width=7.5cm, height=4.5cm]{images/loveda-structure.png}
\centering
\caption{The Structure of LoveDA Dataset}
\label{fig:loveda-structure}
\end{figure}

 Figure \ref{fig:barplot-rural-train}, \ref{fig:barplot-urban-train}, \ref{fig:barplot-rural-test} and \ref{fig:barplot-urban-test} show the bar plot of the  dataset. We observed that there is definitely class imbalanced in all four sets of the data. Rural areas have more agriculture and forest as expected. Urban area have more roads and buildings as the buildings in urban area are bigger and situated closely together while the roads in urban area are generally wider than the ones in rural area. In metropolitan environments, water is frequently present in the form of big rivers, sewers, or lakes, whereas small ponds and ditches are more typical in rural settings. In urban areas, agricultural land is frequently found in the spaces between buildings, whereas in rural areas, agricultural land is more continuous and expansive.

There are two concerning observations from the graphs. The first one being the amount of agriculture in the urban test dataset is larger than expected. the second concern being both rural and urban areas have a very large amount of background area and the background have very different meaning in these two areas. The background in urban area are usually the sidewalks, parking lots and parks while the background in rural area are usually the unused gaps between agricultural area. 

\FloatBarrier

\begin{figure}[!h]
\includegraphics[width=15.0cm, height=8.5cm]{images/rural train barplot.png}
\centering
\caption{Bar plot of the Pixel Counts of the Rural Train Dataset}
\label{fig:barplot-rural-train}
\end{figure}


\begin{figure}[!h]
\includegraphics[width=15.0cm, height=8.5cm]{images/urban train barplot.png}
\centering
\caption{Bar plot of the Pixel Counts of the Rural Train Dataset}
\label{fig:barplot-urban-train}
\end{figure}

\begin{figure}[!h]
\includegraphics[width=15.0cm, height=8.5cm]{images/rural test barplot.png}
\centering
\caption{Bar plot of the Pixel Counts of the Rural Test Dataset}
\label{fig:barplot-rural-test}
\end{figure}

\begin{figure}[!h]
\includegraphics[width=15.0cm, height=8.5cm]{images/urban test barplot.png}
\centering
\caption{Bar plot of the Pixel Counts of the Urban Test Dataset}
\label{fig:barplot-urban-test}
\end{figure}

\FloatBarrier


\section{Data Preprocessing}

As shown in figure \ref{fig:data-uti}, the dataset is split into urban and rural area. Those two categories are further split into training, testing and validation dataset. The mask are only avaialble for the training and validation set. When the actual training is being done, we combined the training and validation set into one folder as shown in figure \ref{fig:loveda-structure}. The model will be trained using both training sets.

\FloatBarrier
\begin{figure}[!h]
\includegraphics[width=10.0cm, height=6.5cm]{images/loveda-chart.png}
\centering
\caption{Diagram of Data Utilization}
\label{fig:data-uti}
\end{figure}
\FloatBarrier

Before training starts, the training images and mask would undergo several transformation. First, they would be cropped into smaller images of 512x512 pixels as required by UNetFormer. Then, those images would be randomly flipped, horizontally and vertically before randomly increasing its brightness.



\section{Baseline Model Design and Implementation}

The baseline model will be written using PyTorch, a deep learning library written in Python. Table \ref{tab:libraries} shows the list of important libraries used to build and test the model. This model would give us a baseline performance and gives us insight on how to proceed for this project.

\begin{table}[]
\centering
\begin{tabular}{|l|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Library}} & \textit{\textbf{Version}} \\ \hline
\textit{\textbf{Pytorch}}              & 1.10                      \\ \hline
\textit{\textbf{Torchvision}}          & 0.11.0                    \\ \hline
\textit{\textbf{Cudatoolkit}}          & 11.3                      \\ \hline
\textit{\textbf{Timm}}                 & -                         \\ \hline
\textit{\textbf{Catalyst}}             & 20.09                     \\ \hline
\textit{\textbf{Albumentations}}       & 1.1.0                     \\ \hline
\textit{\textbf{Numpy}}                & -                         \\ \hline
\textit{\textbf{Opencv-python}}       & -                         \\ \hline
\end{tabular}
\caption{List of Important Libraries Used}
\label{tab:libraries}
\end{table}

The model is trained on a computer with Nvidia GTX 2080 GPU. The semantic segmentation model chosen is UNetFormer \cite{unetformer} and it was discussed in detail in Chapter 2.The code for the model is written in PyTorch and is publicly available at the author's Github page \href{https://github.com/WangLibo1995/GeoSeg}. The model can be summarised in Figure \ref{fig:initial-model}.

\FloatBarrier
\begin{figure}[!h]
\includegraphics[width=13.0cm, height=8.5cm]{images/initial model.png}
\caption{Proposed Baseline Model}
\label{fig:initial-model}
\end{figure}
\FloatBarrier
On the other hand, we trained a U-Net model with ResNet18 backbone using the same dataset to get a comparison with the baseline model. 

Based on Figure \ref{fig:initial-model}, the training image are augmented as described in the previous section. Then, both the training images and masks would be cropped to reduce its size. All the components inside the red line belongs to UNetFormer. Patch embedding would be applied to the cropped images before it is fed into the encoder that is made up of resNet18. The encoder is connected to the decoder through a series of skip connections. The decoder is made up of four Tiny-Swin Transformer. The output from the decoder will be used to calculate the losses and to update the weights. This process is repeated as many times as the desired epoch value. Lastly, after it finishes all epoch, the output will be a collection of cropped black and white images. The mask produced will have a pixel values of 0-7 and this makes it impossible to assess its quality through visual inspection. We converted the 0-7 values to RGB values to make it easier to assess. Table \ref{tab:hyperparam-value} shows the values of hyperparameters used in the model.

\begin{table}[!h]
\centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Hyperparameters}} & \multicolumn{1}{c|}{\textit{\textbf{Description}}} \\ \hline
\textit{\textbf{Learning Rate}}                &   0.0006                                                 \\ \hline
\textit{\textbf{Epoch}}                        &          30                                          \\ \hline
\textit{\textbf{Batch Size}}                   &     16                                               \\ \hline
\textit{\textbf{Optimizer}}                    &         adam                                           \\ \hline
\textit{\textbf{Pruning Rate}}                    &         0                                          \\ \hline

\end{tabular}
\caption{List and Value of Hyperparameters}
\label{tab:hyperparam-value}

\end{table}

\section{Model Evaluation}

The most common evaluation metric used by semantic segmentation is the mean Intersection over Union (mIoU) that is also known as the Jaccard Index or the Jaccard similarity coefficient. The IoU for a single class is defined as:
\begin{equation}
    IoU = \frac{True Positives}{True Positives + False Positives + False Negatives}
\end{equation}

The mIoU is the mean of the IoU for all of the classes.

\section{Model Refinement}

This section would explain the methods that can be taken to refine the baseline model in FYP 2. During FYP 1, we proposed two methods to refine the model. The first method is hyperparameter tuning and the second one is pruning the parameters to reduce its size. 

Hyperparameters are the variables that determines how a network updates its weight. Changing the value of the hyperparameters could result in a vastly different level of performance for the network. Hyperparameter tuning is the process of finding the optimal values of hyperparameters. Table \ref{tab:hyperparam} shows the list and definition of the hyperparameters available to the model.
\begin{table}[!h]
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Hyperparameters}} & \multicolumn{1}{c|}{\textit{\textbf{Description}}}                                                                                                                                                                                                                                                                                           \\ \hline
\textit{\textbf{Learning Rate}}                & \begin{tabular}[c]{@{}l@{}}How quickly the network updates the learned weight depends\\ on the learning rate. A learning rate that is too low would lead\\ to an extremely extended training period that might become stalled.\\ A learning process that is unstable or with suboptimal learned weights can \\result from a learning rate that is too high.\end{tabular} \\ \hline
\textit{\textbf{Epoch}}                        & \begin{tabular}[c]{@{}l@{}}The number of times the training data set will be iterated through\\ the model.\end{tabular}                                                                                                                                                                                                                               \\ \hline
\textit{\textbf{Batch Size}}                   & \begin{tabular}[c]{@{}l@{}}The number of samples that must be processed before the model's\\ parameters are updated.\end{tabular}                                                                                                                                                                                                                    \\ \hline
\end{tabular}
\caption{List and Definition of Hyperparameters}
\label{tab:hyperparam}
\end{table}

For the tuning part, we plan to tune the Transformer decoder using the techniques presented in Chapter 2 and 3.

